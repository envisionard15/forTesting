name: "GTANet"
layer {
  name: "gta_frames_input_layer"
  type: "MemoryData"
  top: "gta_frames_input_layer"
  top: "extraframeoutput"
  memory_data_param
  {
    batch_size: 1
    channels: 3
# Original crop was 227 but IMAGENet is fairly object
# focused and zoomed in compared to GTA, so it would be nice
# to give more area for GTA so the 11x11 filters align
# better with the original training object size. 771 = 11 + 190 * 4
# TODO: Set a per channel image mean and try this ^
    height: 227
    width: 227
  }
  transform_param {
    # Raw data pixels scale by 1 / 255 = 0.00390625
    # scale: 0.00390625
    #mirror::::: true#
    #crop_size: 227#
    #mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
    #mean_value: 104
    #mean_value: 117
    #mean_value: 123
  }
}

#layer {
#  name: "vehicle_states_input_layer"
#  type: "MemoryData"
#  top: "vehicle_states"
#  top: "extravehiclestatesoutput"
#  memory_data_param {
#    # Batch size increases when sleeping vs perceiving.
#    batch_size: 1
#    channels: 6 # Number of output neurons
#    height: 1
#    width: 1
#  }
#}

layer {
  name: "conv1"
  type: "Convolution"
  bottom: "gta_frames_input_layer"
  top: "conv1"
  param {
    lr_mult: 0    # learning rate multiplier for the filters
    decay_mult: 0 # weight decay multiplier for the filters
  }
  param {
    lr_mult: 0    # learning rate multiplier for the biases
    decay_mult: 0 # weight decay multiplier for the biases
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
	bias_filler {
	  type: "constant"
	  value: 0
	}
  }
}


# blobs_lr increases as we move up the net to retrain
# task (IMAGENet) specific features for driving.

layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}

layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}

layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}

layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    lr_mult: 0    # learning rate multiplier for the filters
    decay_mult: 0 # weight decay multiplier for the filters
  }
  param {
    lr_mult: 0    # learning rate multiplier for the biases
    decay_mult: 0 # weight decay multiplier for the biases
  }

  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}

layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}

layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}

layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}

layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
 param {
   lr_mult: 0.01    # learning rate multiplier for the filters
   decay_mult: 0.01 # weight decay multiplier for the filters
 }
 param {
   lr_mult: 0.02  # learning rate multiplier for the biases
   decay_mult: 0  # weight decay multiplier for the biases
 }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}

layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    lr_mult: 0.02    # learning rate multiplier for the filters
    decay_mult: 0.02 # weight decay multiplier for the filters
  }
  param {
    lr_mult: 0.04    # learning rate multiplier for the biases
    decay_mult: 0    # weight decay multiplier for the biases
  }
   convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}

layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}

layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
 param {
   lr_mult: 0.04    # learning rate multiplier for the filters
   decay_mult: 0.04 # weight decay multiplier for the filters
 }
 param {
   lr_mult: 0.08    # learning rate multiplier for the biases
   decay_mult: 0 # weight decay multiplier for the biases
 }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}

layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}

layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}

layer {
  name: "fc6_gtanet"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1     # learning rate multiplier for the filters
    decay_mult: 1  # weight decay multiplier for the filters
  }
  param {
    lr_mult: 2     # learning rate multiplier for the biases
    decay_mult: 0  # weight decay multiplier for the biases
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}

layer {
  name: "relu6_gtanet"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}

layer {
  name: "drop6_gtanet"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}

#layer {
#  name: "reshape"
#  type: "Reshape"
#  bottom: "vehicle_states"
#  top: "vehicle_states_reshape"
#  reshape_param {
#    shape {
## Mirror batch size dimension with 0
#      dim: 0
#      dim: 6
#    }
#  }
#}

#layer {
#  name: "concat_vehicle_state"
#  bottom: "fc6"
#  bottom: "vehicle_states_reshape"
#  top: "fc6_w_vehicle_states"
#  type: "Concat"
#}

layer {
  name: "fc7_gtanet"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1    # learning rate multiplier for the filters
    decay_mult: 1 # weight decay multiplier for the filters
  }
  param {
    lr_mult: 2    # learning rate multiplier for the biases
    decay_mult: 0 # weight decay multiplier for the biases
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}

layer {
  name: "relu7_gtanet"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}

layer {
  name: "drop7_gtanet"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.05
  }
}

# The output layer is a fully-connected linear layer with a single output for each valid action.
layer {
  name: "gtanet_fctop"
  type: "InnerProduct"
  bottom: "fc7"
  top: "gtanet_fctop"
  param {
    lr_mult: 1    # learning rate multiplier for the filters
    decay_mult: 1 # weight decay multiplier for the filters
  }
  param {
    lr_mult: 2    # learning rate multiplier for the biases
    decay_mult: 0 # weight decay multiplier for the biases
  }
  inner_product_param {
    num_output: 6 # Number of output neurons
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

layer {
  name: "target_input_layer"
  type: "MemoryData"
  top: "target"
  top: "extratargetoutput"
  memory_data_param {
    # Batch size increases when sleeping vs perceiving.
    batch_size: 1
    channels: 6 # Number of output neurons
    height: 1
    width: 1
  }
}

#layer {
#  name: "silence_layer"
#  type: "Silence"
#  bottom: "extraframeoutput"
#  bottom: "extratargetoutput"
#}

#layer {
#  name: "reshape"
#  type: "Reshape"
#  bottom: "target"
#  top: "target_reshape"
#  reshape_param {
#    shape {
## Mirror batch size dimension with 0
#      dim: 0
#      dim: 1
#    }
#  }
#}

#layer {
#  name: "reshape"
#  type: "Reshape"
#  bottom: "gtanet_fctop"
#  top: "gtanet_fctop_reshape"
#  reshape_param {
#    shape {
## Mirror batch size dimension with 0
#      dim: 0
#      dim: 9
#      dim: 1
#      dim: 1
#    }
#  }
#}

#layer {
#  name: "loss"
#  type: "SoftmaxWithLoss"
#  bottom: "gtanet_fctop"
#  bottom: "target"
#  top: "loss"
#}

layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "gtanet_fctop"
  bottom: "target"
  top: "loss"
}

# DriveNET 1 month IMAGENet augmented dataset training with inception architecture
# Cityscape dataset with Daimler


#######################################
# Caffenet layers we are not using
#######################################

#name: "CaffeNet"

#### Caffenet original train data in - not using
#layers {
#  name: "data"
#  type: "Data"
#  top: "data"
#  top: "label"
#  include {
#    phase: TRAIN
#  }
#  transform_param {
#    mirror: true
#    crop_size: 227
#    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
#  }
## mean pixel / channel-wise mean instead of mean image
##  transform_param {
##    crop_size: 227
##    mean_value: 104
##    mean_value: 117
##    mean_value: 123
##    mirror: true
##  }
#  data_param {
#    source: "examples/imagenet/ilsvrc12_train_lmdb"
#    batch_size: 256
#    backend: LMDB
#  }
#}

# Caffenet original test data in - not using
#layers {
#  name: "data"
#  type: "Data"
#  top: "data"
#  top: "label"
#  include {
#    phase: TEST
#  }
#  transform_param {
#    mirror: false
#    crop_size: 227
#    mean_file: "data/ilsvrc12/imagenet_mean.binaryproto"
#  }
## mean pixel / channel-wise mean instead of mean image
##  transform_param {
##    crop_size: 227
##    mean_value: 104
##    mean_value: 117
##    mean_value: 123
##    mirror: false
##  }
#  data_param {
#    source: "examples/imagenet/ilsvrc12_val_lmdb"
#    batch_size: 50
#    backend: LMDB
#  }
#}
# layers {
#   name: "fc8"
#   type: INNER_PRODUCT
#   bottom: "fc7"
#   top: "fc8"
#   param {
#     lr_mult: 1
#     decay_mult: 1
#   }
#   param {
#     lr_mult: 2
#     decay_mult: 0
#   }
#   inner_product_param {
#     num_output: 1000
#     weight_filler {
#       type: "gaussian"
#       std: 0.01
#     }
#     bias_filler {
#       type: "constant"
#       value: 0
#     }
#   }
# }
#
# layers {
#   name: "accuracy"
#   type: "Accuracy"
#   bottom: "fc8"
#   bottom: "label"
#   top: "accuracy"
#   include {
#     phase: TEST
#   }
# }
#
# layers {
#   name: "loss"
#   type: "SoftmaxWithLoss"
#   bottom: "fc8"
#   bottom: "label"
#   top: "loss"
# }
#######################################
# End Caffenet layers we are not using
#######################################
